<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Deep Dive: Our Teachable Machine</title>
    <link rel="stylesheet" href="stylepage.css">
</head>
<body>
    <header>
        <a href="index.html"><img src="Frame 1.svg" width="75" height="75"></a>
        <h1>Our Teachable Machine: Rock, Paper, Scissors</h1>
        <!--adding icons to the header-->
        <div class="header-icons">
            <a href="about.html"><img src="our project statement icon.svg" width="75" height="75"></a>
            <a href="resources.html"><img src="teachable machine icon.svg" width="75" height="75"></a>
        </div>
        
    </header>
    <!--Navigation bar-->
    <div class="content-wrapper">
        <nav>
            <ul>
                <li><a href="index.html" class="hello">Home</a></li>
                <li><a href="#introduction">What is a Teachable Machine?</a></li>
                <li><a href="#key features">Key Features</a></li>
                <li><a href="#rps">Why Rock, Paper, Scissors?</a></li>
                <li><a href="#use-teachable-machine">Use Our Machine!</a></li>
                <li><a href="#more">Using a Teachable Machine</a></li>
                <li><a href="#resources">Resources</a></li>
            </ul>
        </nav>
        <main>
            <!-- Intro to our teachable machine-->
            <section id="introduction">
                <h2>What is a Teachable Machine?</h2>
                <p>Teachable Machine is a web-based tool by Google designed to make machine learning (ML) accessible to everyone, regardless of technical expertise. It allows users to create custom classification models for tasks like recognizing images, sounds, and poses without coding.</p>
                <div class="video-container">
                    <iframe width="560" height="315" src="https://www.youtube.com/embed/T2qQGqZxkD0?si=-ieOTGJC0IDdh8H8" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
                </div>
                <!--explaining why these machines are important-->
                <h2>Why do you care about teachable machines?</h2>
                <p>Teachable Machine is an important tool because it democratizes access to machine learning, empowering individuals, educators, artists, and small businesses to engage with AI without requiring specialized technical skills. By providing an intuitive platform for training models that recognize images, sounds, and poses, it fosters innovation and creativity across diverse fields. In education, it allows students to explore AI concepts interactively, making learning more engaging and relevant. For creators and entrepreneurs, it enables rapid prototyping and experimentation, opening new possibilities for interactive projects, accessibility tools, and user-centric designs. Furthermore, by lowering the barriers to AI, Teachable Machine encourages inclusivity, allowing underrepresented groups and smaller organizations to participate in shaping technology. As AI becomes a cornerstone of modern life, tools like this ensure broader participation, enhance technological literacy, and enable communities worldwide to leverage AI for societal benefit.</p>
                
                <p><a href="https://sjcoombsnovii.github.io/about.html" target="_blank">Visit our project statement to learn more about our approach based on Joy Buolamwini’s Unmasking AI.</a></p>
            </section>
            <!--Adding a key features section of the teachable machine-->
            <section id="key features">
                <h2>Key Features</h2>
                <p>Ease of Use:</p>
                <ul>
                    <li>A simple graphical user interface enables users to build models in a matter of minutes by providing training data (images, audio, or poses).</li>
                </ul>
                <p>Support for Classification Tasks:</p>
                <ul>
                    <li>Image Classification: Detect and classify objects or scenes.</li>
                    <li>Audio Classification: Recognize sounds or speech patterns.</li>
                    <li>Pose Detection: Detect and classify body poses or movements.</li>
                </ul>
                <p>How it works:</p>
                <ul>
                    <li>Gather Data: Collect images, sounds, or pose data through uploads or live input.</li>
                    <li>Train the Model: Assign data to classes (e.g., "cat" vs. "dog") and let the tool train the model.</li>
                    <li>Test the Model: Validate its accuracy with new data.</li>
                    <li>Use the Model: Export or deploy the trained model for real-world use.</li>
                </ul>
               
            </section>

            <section id="rps">
                <!--Our Machine-->
                <h2>Rock, Paper, Scissors</h2>
                <p>The idea of keeping autonomy with unpredictability.</p>
                <div class="video-container">
                    <iframe src="//content.jwplatform.com/players/4UU4jtsz-FvQKszTI.html" width="640" height="360" frameborder="0" scrolling="auto"></iframe>
                </div>
                <p>As mentioned, the best strategy in Rock Paper Scissors is to be unpredictable. However, humans are not naturally unpredictable—there’s always some level of bias or intuition at play, as we’ve explored in LIS 500. When teaching our machine, we acknowledge its predictability—it does what we train it to do. Yet, paradoxically, this same machine can be unpredictable. Sometimes, it might interpret a rock when you’re holding up paper. In this way, machine learning can be more unpredictable than humans. The less you train a machine, the more unpredictable it becomes.</p>
                <p>We’re training machines to be like humans, which means we must provide a robust dataset to achieve this. We find it fascinating how humans strive to be unpredictable, yet expect machines to be completely predictable. This brings us back to the classic game of Rock, Paper, Scissors, where humans often try to mimic the randomness of an untrained machine.</p>
            </section>

           
           <!--Used the Resources main section to split the cards horizontally to break up the page-->
            <main class="resources-main">
                <section id="use-teachable-machine">
                    <!-- Adding in the Java Script that came from the teachable machine website to be able to use on our website.-->
                <h2>Use Our Teachable Machine!</h2>
                <p>Using a blank background, but your hands up & show rock, paper, or scissors to play!</p>
                

                <!--adding in our teachable machine to this card!-->
                <p> </p>
                <p>Press Start to Begin.</p>
                <button type="button" onclick="init()">Start</button>
                <div id="webcam-container"></div>
                <div id="label-container"></div>
                <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest/dist/tf.min.js"></script>
                <script src="https://cdn.jsdelivr.net/npm/@teachablemachine/image@latest/dist/teachablemachine-image.min.js"></script>
                <script type="text/javascript">
                // More API functions here:
                // https://github.com/googlecreativelab/teachablemachine-community/tree/master/libraries/image

                // the link to your model provided by Teachable Machine export panel
                const URL = "https://teachablemachine.withgoogle.com/models/9a6umZfl9/";

                let model, webcam, labelContainer, maxPredictions;

                // Load the image model and setup the webcam
                async function init() {
                    const modelURL = URL + "model.json";
                    const metadataURL = URL + "metadata.json";

                // load the model and metadata
                // Refer to tmImage.loadFromFiles() in the API to support files from a file picker
                 // or files from your local hard drive
                 // Note: the pose library adds "tmImage" object to your window (window.tmImage)
                 model = await tmImage.load(modelURL, metadataURL);
                 maxPredictions = model.getTotalClasses();

                 // Convenience function to setup a webcam
                 const flip = true; // whether to flip the webcam
                webcam = new tmImage.Webcam(200, 200, flip); // width, height, flip
                await webcam.setup(); // request access to the webcam
                await webcam.play();
                window.requestAnimationFrame(loop);

                // append elements to the DOM
                 document.getElementById("webcam-container").appendChild(webcam.canvas);
                labelContainer = document.getElementById("label-container");
                for (let i = 0; i < maxPredictions; i++) { // and class labels
                    labelContainer.appendChild(document.createElement("div"));
                }
                }

                async function loop() {
                    webcam.update(); // update the webcam frame
                    await predict();
                    window.requestAnimationFrame(loop);
                }

                // run the webcam image through the image model
                async function predict() {
                    // predict can take in an image, video or canvas html element
                    const prediction = await model.predict(webcam.canvas);
                    for (let i = 0; i < maxPredictions; i++) {
                        const classPrediction =
                            prediction[i].className + ": " + prediction[i].probability.toFixed(2);
                            labelContainer.childNodes[i].innerHTML = classPrediction;
                    }
                }
            </script>
                </section>
                <section id="ourmachine">
                   <!--Adding Youtube Short (via a workaround from: https://docs.document360.com/docs/embed-youtube-shorts) of our Machine-->
                <iframe width="315" height="560" 
                src="https://youtube.com/embed/cgeEMBYKHGY?feature=shared" 
                title="YouTube video player" frameborder="0" 
                allow="accelerometer; autoplay; clipboard-write; encrypted-media;
                gyroscope; picture-in-picture;
                web-share"
                allowfullscreen>
            </iframe>
                </section>
            </main>
        
            <section id="more">
                <!-- Wanted to have a walkthrough on how to set one up, and when it could be useful to do so -->
                <h2>Using a Teachable Machine</h2>
                <div class="data-image">
                    <img src="Data.png" width="1000" height="250">
                </div>
                <p>Here is a video walkthrough of how we set up our teachable machine:</p>
                <iframe width="560" height="315" src="https://www.youtube.com/embed/JgCV1N6LNp8?si=zIriIcTjJyYRlWRa" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
                <div class="achieve">
                    <p>When could it be useful to set up your own teachable machine?</p>
                    <ul>
                     <li>Education: A practical way to introduce students to AI and machine learning.</li>
                     <li>Prototyping: Test AI ideas before full-scale implementation.</li>
                     <li>Accessibility Solutions: Build gesture or voice-controlled interfaces.</li>
                     <li>Creative Projects: Develop interactive art installations.</li>
                     </ul>
                </div>
                
            </section>
            

            <section id="resources">
                <!-- Extra resources & sources for our research -->
                <h2>Additional Resources</h2>
                <ul>
                    <li><a href="http://research.google/pubs/teachable-machine-approachable-web-based-tool-for-exploring-machine-learning-classification/" target="_blank">Google Research: "Teachable Machine: Approachable Web-Based Tool" – Explains the rationale and design goals behind Teachable Machine.</a></li>
                    <li><a href="https://teachablemachine.withgoogle.com/faq" target="_blank">Teachable Machine FAQ: "How does Teachable Machine work?" – Official FAQ detailing its functionality and use cases.</a></li>
                    <li><a href="https://www.pluralsight.com/resources/blog/guides/getting-started-with-google-teachable-machine" target="_blank">Pluralsight Blog: "Getting Started with Teachable Machine" – Provides a step-by-step guide to using Teachable Machine effectively.</a></li>
                    <li><a href="https://www.commonsense.org/education/reviews/teachable-machine" target="_blank">Common Sense Education Review: "Teachable Machine Review for Teachers" – Analyzes the tool's utility in educational settings, highlighting its strengths and limitations.</a></li>
                    <li><a href="https://www.verywellmind.com/implicit-bias-overview-4178401" target="_blank">How Does Implicit Bias Influence Behavior?</a></li>
                    <li><a href="https://github.com/sjcoombsnovii/sjcoombsnovii.github.io" target="_blank">Our Git Repo</a></li>
                </ul>
            </section>
        </main>
    </div>
    <footer>
        <p> LIS 500 Amar Algogandi & Shayla Coombs </p>
    </footer>
</body>
</html>